{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksportowanie modelu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import json\n",
    "from keras.models import Model\n",
    "from tensorflow import keras\n",
    "import time"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "#model_name = \"my_ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8\"\n",
    "#model_name = \"my_ssd_resnet50_v1_fpn\"\n",
    "#model_name = \"my_ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\"\n",
    "#model_name = \"my_ssd_resnet50_v1_fpn_exported\"\n",
    "#model_name = \"my_ssd_resnet152_v1_fpn_640x640_coco17_tpu-8\"\n",
    "#model_name = \"my_ssd_resnet101_v1_fpn_640x640_coco17_tpu-8\"\n",
    "model_name = \"my_ssd_resnet101_v1_fpn_640x640_coco17_tpu-8_2\"\n",
    "\n",
    "keras_model_name = f\"{model_name}_keras\"\n",
    "\n",
    "images_path = f\"TensorFlow/workspace/training_demo/images\"\n",
    "model_path = f\"TensorFlow/workspace/training_demo/models/{model_name}\"\n",
    "exported_model_path = f\"TensorFlow/workspace/training_demo/exported-models/\"\n",
    "\n",
    "keras_model_path = f\"TensorFlow/workspace/training_demo/models/{keras_model_name}\"\n",
    "weights_path = f\"{keras_model_path}/weights\"\n",
    "npy_weights_path = f\"{keras_model_path}/npy_weights\"\n",
    "\n",
    "weights_filename = f\"{keras_model_name}_weights.h5\"\n",
    "model_filename = f\"{keras_model_name}.h5\"\n",
    "\n",
    "ckpt_dict = {\"my_ssd_resnet50_v1_fpn\":'/ckpt-31',\n",
    "                    \"my_ssd_resnet101_v1_fpn_640x640_coco17_tpu-8\":\"/ckpt-28\",\n",
    "                    \"my_ssd_resnet152_v1_fpn_640x640_coco17_tpu-8\":\"/ckpt-26\",\n",
    "                    \"my_ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8\":\"/ckpt-26\",\n",
    "                    \"my_ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\": \"/ckpt-51\"}\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksportowanie wytrenowanego modelu do formatu Tensorflow\n",
    "za pomocą TF Object Detection API, freezing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "export_model_command = f\"python TensorFlow/workspace/training_demo/exporter_main_v2.py --input_type image_tensor --pipeline_config_path {model_path}/pipeline.config --trained_checkpoint_dir {model_path}/ --output_directory {exported_model_path}/{model_name}_exported\"\n",
    "print(export_model_command)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "!{export_model_command}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksportowanie wytrenowanego modelu do formatu Keras\n",
    "\n",
    "Wczytywanie modelu z TF Saved Model lub z Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "if model_name[-9:] == \"_exported\":\n",
    "    print(\"[Loading TF2 Saved Model]\")\n",
    "    print(f'Loading model - {model_name}...')\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = tf.saved_model.load(exported_model_path + f\"{model_name}/saved_model\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'The model has been loaded - {elapsed_time:.2f}s\\n')\n",
    "else:\n",
    "    tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)\n",
    "    print(\"[Loading TF2 Checkpoint]\")\n",
    "    print(f'\\nLoading model - {model_name}...')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load pipeline config and build a detection model\n",
    "    configs = config_util.get_configs_from_pipeline_file(model_path + \"/pipeline.config\")\n",
    "    model_config = configs['model']\n",
    "\n",
    "    model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "    # Restore checkpoint\n",
    "    ckpt = tf.compat.v2.train.Checkpoint(model=model)\n",
    "    ckpt.restore(model_path + ckpt_dict[model_name]).expect_partial()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'The model has been loaded - {elapsed_time:.2f}s\\n')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponowne zapisanie modelu w formacie TF Saved Model, obsługującym Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "tf.saved_model.save(model, exported_model_path + f\"{model_name}_keras\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przenoszenie modelu z Tensorflow Object Detection API na model KERAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "from object_detection.meta_architectures import ssd_meta_arch\n",
    "\n",
    "class CustomSSDMetaArch(ssd_meta_arch.SSDMetaArch):\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Dodaj inne parametry konstrukcji, które są potrzebne do odtworzenia instancji.\n",
    "        config.update({\n",
    "            'other_arg1': self.other_arg1,\n",
    "            'other_arg2': self.other_arg2,\n",
    "            # Dodaj inne atrybuty tutaj\n",
    "        })\n",
    "        return config"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "# Ścieżka do pliku config modelu\n",
    "pipeline_config = model_path + \"/pipeline.config\"\n",
    "# Załaduj konfigurację\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "# Zamień instancję SSDMetaArch na CustomSSDMetaArch\n",
    "detection_model.__class__ = CustomSSDMetaArch\n",
    "\n",
    "# Przywróć checkpoint modelu\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(model_path + '/' + ckpt_dict[model_name]).expect_partial()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "class KerasObjectDetectionModel(tf.keras.Model):\n",
    "    def __init__(self, detection_model, **kwargs):\n",
    "        super(KerasObjectDetectionModel, self).__init__(**kwargs)\n",
    "        self.detection_model = detection_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        preprocessed_inputs, shapes = self.detection_model.preprocess(inputs)\n",
    "        prediction_dict = self.detection_model.predict(preprocessed_inputs, shapes)\n",
    "        detections = self.detection_model.postprocess(prediction_dict, shapes)\n",
    "        return detections\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(KerasObjectDetectionModel, self).get_config()\n",
    "        config.update({\n",
    "            \"detection_model\": self.detection_model\n",
    "        })\n",
    "        return config"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "# Utwórz instancję modelu Keras\n",
    "keras_model = KerasObjectDetectionModel(detection_model)\n",
    "\n",
    "# Określ kształt wejściowy, używając warstwy Input i Model\n",
    "input_shape = (640, 640, 3)  # Dostosuj wysokość i szerokość do rozmiaru wejściowego twojego modelu\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "outputs = keras_model(inputs)\n",
    "\n",
    "# Stwórz nowy model Keras z określonymi wejściami i wyjściami\n",
    "final_keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Zapisz model\n",
    "final_keras_model.save(exported_model_path + f\"{keras_model_name}_v2\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "final_keras_model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract layer weights from TF checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "# regex for renaming the tensors to their corresponding Keras counterpart\n",
    "re_repeat = re.compile(r'Repeat_[0-9_]*b')\n",
    "re_block8 = re.compile(r'Block8_[A-Za-z]')\n",
    "\n",
    "\n",
    "def get_filename(key):\n",
    "    filename = str(key)\n",
    "    filename = filename.replace('/', '_')\n",
    "    filename = filename.replace(model_name + \"_\", '')\n",
    "\n",
    "    # remove \"Repeat\" scope from filename\n",
    "    filename = re_repeat.sub('B', filename)\n",
    "\n",
    "    if re_block8.match(filename):\n",
    "        # the last block8 has different name with the previous 9 occurrences\n",
    "        filename = filename.replace('Block8', 'Block8_10')\n",
    "    elif filename.startswith('Logits'):\n",
    "        # remove duplicate \"Logits\" scope\n",
    "        filename = filename.replace('Logits_', '', 1)\n",
    "\n",
    "    # from TF to Keras naming\n",
    "    filename = filename.replace('_weights', '_kernel')\n",
    "    filename = filename.replace('_biases', '_bias')\n",
    "\n",
    "    return filename + '.npy'\n",
    "\n",
    "if not os.path.exists(npy_weights_path):\n",
    "    os.makedirs(npy_weights_path)\n",
    "\n",
    "reader = tf.train.load_checkpoint(exported_model_path + \"/\" + model_name + \"_exported/checkpoint/\")\n",
    "\n",
    "for key in reader.get_variable_to_shape_map():\n",
    "    # not saving the following tensors\n",
    "    if key == 'global_step':\n",
    "        continue\n",
    "    if 'AuxLogit' in key:\n",
    "        continue\n",
    "\n",
    "    # convert tensor name into the corresponding Keras layer weight name and save \n",
    "    arr = reader.get_tensor(key)\n",
    "    tensor_path = npy_weights_path + \"/\" + get_filename(key)\n",
    "    np.save(tensor_path, arr)\n",
    "    print(\"tensor_name: \", key)\n",
    "    #print(\"tensor_filename: \", get_filename(key))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "source": [
    "print('Instantiating an empty ResNet50V2 model...')\n",
    "model = keras.applications.ResNet50V2(\n",
    "    weights=None,\n",
    "    input_shape= (640, 640, 3), # Rozmiar modelu 640x640\n",
    "    classes = 1,\n",
    "    classifier_activation= 'softmax',\n",
    ")\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load NumPy weight files and save to a Keras HDF5 weights file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "print('Loading numpy weights from', npy_weights_path)\n",
    "weights_names = []\n",
    "for layer in model.layers:\n",
    "    if layer.weights:\n",
    "        weights = []\n",
    "        \n",
    "        for w in layer.weights:\n",
    "            weight_name = os.path.basename(w.name).replace(':0', '')\n",
    "            weight_file = layer.name + '_' + weight_name + '.npy'\n",
    "            print(f\"{layer.name} || {weight_name} || {weight_file}\")\n",
    "            weight_arr = np.load(os.path.join(npy_weights_path, weight_file))\n",
    "            weights_names.append(weight_file)\n",
    "            weights.append(weight_arr)\n",
    "        layer.set_weights(weights)\n",
    "\n",
    "print('Saving weights...')\n",
    "model.save_weights(os.path.join(weights_path, weights_filename))\n",
    "print('Saving model...')\n",
    "model.save(os.path.join(model_path, model_filename))"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_od",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
